{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re, string, unicodedata\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from textblob import TextBlob\n",
    "from textblob import Word\n",
    "import sys\n",
    "\n",
    "dataset = pd.read_csv('/home/divyampramod-d/Projects/Mini_Projects/IMDB_Dataset.csv')\n",
    "# tokenizer = ToktokTokenizer()\n",
    "# nltk.download('stopwords')\n",
    "# stopword_list = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "# #Remove the html strips from the dataset\n",
    "# def strip_html(text):\n",
    "#     soup = BeautifulSoup(text, \"html.parser\")\n",
    "#     return soup.get_text()\n",
    "\n",
    "# #Remove Square brackets within the text\n",
    "# def remove_between_square_brackets(text):\n",
    "#     return re.sub('\\[[^]]*\\]', '', text)\n",
    "\n",
    "# #Removing any noise in the text\n",
    "# def remove_noise(text):\n",
    "#     text = strip_html(text)\n",
    "#     text = remove_between_square_brackets(text)\n",
    "#     return text\n",
    "\n",
    "# dataset['review'] = dataset['review'].apply(remove_noise)\n",
    "\n",
    "# #Removing any special characters in the dataset\n",
    "# def remove_special_characters(text, remove_digits=True):\n",
    "#     pattern = r'[^a-zA-z0-9\\s]'\n",
    "#     text = re.sub(pattern, '', text)\n",
    "#     return text\n",
    "\n",
    "# dataset['review'] = dataset['review'].apply(remove_special_characters)\n",
    "\n",
    "# print(dataset['review'][0])\n",
    "\n",
    "tokens = list(map(lambda x : set(x.split(\" \")), dataset['review']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set()\n",
    "for sent in tokens:\n",
    "    for word in sent:\n",
    "        if(len(word) > 0):\n",
    "            vocab.add(word)\n",
    "vocab = list(vocab)\n",
    "word2index = {}\n",
    "for i, word in enumerate(vocab):\n",
    "    word2index[word] = i\n",
    "\n",
    "input_dataset = list()\n",
    "for sent in tokens:\n",
    "    sent_indices = list()\n",
    "    for word in sent:\n",
    "        try:\n",
    "            sent_indices.append(word2index[word])\n",
    "        except:\n",
    "            \"\"\n",
    "    input_dataset.append(list(set(sent_indices)))\n",
    "target_dataset = list()\n",
    "\n",
    "for label in dataset['sentiment']:\n",
    "    if label == 'positive':\n",
    "        target_dataset.append(1)\n",
    "    else:\n",
    "        target_dataset.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "439838\n",
      "Iter:0 Progress:97.99% Training Accuracy:0.8393673469387755%\n",
      "Iter:1 Progress:97.99% Training Accuracy:0.8735816326530612%\n",
      "Test Accuracy:0.885\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "alpha, iterations = (0.01, 2)\n",
    "hidden_size = 100\n",
    "\n",
    "weights_0_1 = 0.2*np.random.random((len(vocab), hidden_size)) - 0.1\n",
    "weights_1_2 = 0.2 * np.random.random((hidden_size, 1)) - 0.1\n",
    "\n",
    "correct, total = (0, 0)\n",
    "for iter in range(iterations):\n",
    "\n",
    "    for i in range(len(input_dataset) - 1000):\n",
    "        \n",
    "        x, y = (input_dataset[i], target_dataset[i])\n",
    "        layer_1 = sigmoid(np.sum(weights_0_1[x], axis = 0))\n",
    "        layer_2 = sigmoid(np.dot(layer_1, weights_1_2))\n",
    "\n",
    "        layer_2_delta = layer_2 - y\n",
    "        layer_1_delta = layer_2_delta.dot(weights_1_2.T)\n",
    "        \n",
    "        weights_0_1[x] -= layer_1_delta * alpha\n",
    "        weights_1_2 -= np.outer(layer_1, layer_2_delta) * alpha\n",
    "\n",
    "        if(np.abs(layer_2_delta) < 0.5):\n",
    "            correct += 1\n",
    "        \n",
    "        total += 1\n",
    "        if (i % 10 == 9):\n",
    "            progress = str(i / float(len(input_dataset)))\n",
    "            sys.stdout.write('\\rIter:' + str(iter) + ' Progress:' + progress[2:4]\\\n",
    "                             +'.'+progress[4:6]\\\n",
    "                             +'% Training Accuracy:'\\\n",
    "                             + str(correct/float(total)) + '%')\n",
    "    print()\n",
    "\n",
    "correct, total = (0, 0)\n",
    "for i in range(len(input_dataset) - 1000, len(input_dataset)):\n",
    "    x = input_dataset[i]\n",
    "    y = target_dataset[i]\n",
    "\n",
    "    layer_1 = sigmoid(np.sum(weights_0_1[x], axis = 0))\n",
    "    layer_2 = sigmoid(layer_1.dot(weights_1_2))\n",
    "\n",
    "    if (np.abs(layer_2 - y) < 0.5):\n",
    "        correct += 1\n",
    "    total += 1\n",
    "\n",
    "print(\"Test Accuracy:\" + str(correct / float(total)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, random, math\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(1)\n",
    "random.seed(1)\n",
    "tokens = list(map(lambda x: (x.split(\" \")), dataset['review']))\n",
    "wordcnt = Counter()\n",
    "for sent in tokens:\n",
    "    for word in sent:\n",
    "        wordcnt[word] -= 1\n",
    "vocab = list(set(map(lambda x : x[0], wordcnt.most_common())))\n",
    "\n",
    "word2index = {}\n",
    "for i, word in enumerate(vocab):\n",
    "    word2index[word] = i\n",
    "\n",
    "concatenated = list()\n",
    "input_dataset = list()\n",
    "for sent in tokens:\n",
    "    sent_indices = list()\n",
    "    for word in sent:\n",
    "        try:\n",
    "            sent_indices.append(word2index[word])\n",
    "            concatenated.append(word2index[word])\n",
    "        except:\n",
    "            \"\"\n",
    "    input_dataset.append(sent_indices)\n",
    "concatenated = np.array(concatenated)\n",
    "random.shuffle(input_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11557297,)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha, iterations = (0.05, 2)\n",
    "hidden_size, window, negative = (50, 2, 5)\n",
    "\n",
    "weights_0_1 = (np.random.rand(len(vocab), hidden_size) - 0.5) * 0.2\n",
    "weights_1_2 = np.random.rand(len(vocab), hidden_size) * 0\n",
    "\n",
    "layer_2_target = np.zeros(negative+1)\n",
    "layer_2_target[0] = 1\n",
    "\n",
    "def similar(target = 'beautiful'):\n",
    "    target_index = word2index[target]\n",
    "\n",
    "    scores = Counter()\n",
    "\n",
    "    for word, index in word2index.items():\n",
    "        raw_difference = weights_0_1[index] - (weights_0_1[target_index])\n",
    "        squared_difference = weights_0_1 * raw_difference\n",
    "        scores[word] = -math.sqrt(sum(squared_difference))\n",
    "    return scores.most_common(10)\n",
    "\n",
    "for rev_i, review in enumerate(input_dataset * iterations):\n",
    "    for target_i in range(len(review)):\n",
    "        target_samples = [review[target_i]] + list(concatenated[(np.random.rand(negative) * len(concatenated)).astype('int').tolist()])\n",
    "\n",
    "        left_context = review[max(0, target_i - window) : target_i]\n",
    "        right_context = review[target_i + 1 : min(len(review), target_i + window)]\n",
    "\n",
    "        layer_1 = np.mean(weights_0_1[left_context + right_context], axis = 0)\n",
    "        layer_2 = sigmoid(layer_1.dot(weights_1_2[target_samples].T))\n",
    "        layer_2_delta = layer_2 - layer_2_target\n",
    "        layer_1_delta = layer_2_delta.dot(weights_1_2[target_samples])\n",
    "\n",
    "        weights_0_1[left_context+right_context] -= layer_1_delta * alpha\n",
    "        weights_1_2[target_samples] -= np.outer(layer_2_delta, layer_1) * alpha\n",
    "        \n",
    "    if(rev_i % 250 == 0):\n",
    "        sys.stdout.write('\\rProgresss:' + str(rev_i / float(len(input_dataset) * iterations)) + \" \" + str(similar('terrible')))\n",
    "        sys.stdout.write('\\rProgress:' + str(rev_i/float(len(input_dataset) * iterations)))\n",
    "print(similar('terrible'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
